## 概率论（probability theory)
https://time.geekbang.org/column/article/1341

### 古典概率模型
$$P(A)=k / n $$
可以推导出复杂的随机事件的概率

### 条件概率（conditional probability）
$$P(A∣B)=P(AB)/P(B)$$​
$P(AB)$ 称为联合概率（joint probability），表示的是 A 和 B 两个事件共同发生的概率。
对于相互独立的事件：
$P(AB)=P(A)⋅P(B)$
$P(A∣B)=P(A)$

### 全概率公式（law of total probability）=
$$P(A) = \sum_{i=1}^{N}P(A \mid B_{i}) \cdot P(B_{i})$$
$$\sum_{i=1}^{N}P(B_{i})=1$$
转化为在不同情况下发生的简单事件的概率求和。
先做出一些假设$（P(B_{i}​)）$，再在这些假设下讨论随机事件的概率$（P(A∣B_{i}​)）$。

### 逆概率
在事件结果已经确定的条件下$（P(A)）$，推断各种假设发生的可能性$（P(Bi_{i}∣A)）$

### 贝叶斯公式（Bayes' theorem）
$$P(B_{i}\mid A)=\frac{P(A \mid B_{i}) \cdot P(B_{i})}{\sum_{j=1}^{N}P(A \mid B_{j}) \cdot P(B_{j})}$$

贝叶斯学派眼中，概率描述的是随机事件的可信程度

贝叶斯公式可以进一步抽象为贝叶斯定理（Bayes' theorem）

### 贝叶斯定理（Bayes' theorem）
$$P(H \mid D) = \frac{P(D \mid H) \cdot P(H)}{{(D)}}$$
$P(H)$ 被称为先验概率（prior probability），
即预先设定的假设成立的概率；

$P(D \mid H)$ 被称为似然概率（likelihood function），
是在假设成立的前提下观测到结果的概率；

$P(H \mid D)$ 被称为后验概率（posterior probability），即在观测到结果的前提下假设成立的概率。

从科学研究的方法论来看，贝叶斯定理提供了一种全新的逻辑。它根据观测结果寻找合理的假设，或者说根据观测数据寻找最佳的理论解释，其关注的焦点在于后验概率。概率论的贝叶斯学派（Bayesian probability）正是诞生于这种理念。

### 概率的估计方法
1. 最大似然估计法（maximum likelihood estimation）
使训练数据出现的概率最大化，依此确定概率分布中的未知参数，估计出的概率分布也就最符合训练数据的分布
2. 最大后验概率法（maximum a posteriori estimation）
最大后验概率法的思想则是根据训练数据和已知的其他条件，使未知参数出现的可能性最大化，并选取最可能的未知参数取值作为估计值。

在估计参数时，最大似然估计法只需要使用训练数据，最大后验概率法除了数据外还需要额外的信息，就是贝叶斯公式中的先验概率。
两者分别体现出频率学派和贝叶斯学派对概率的理解方式。

### 随机变量（random variable)
1. 离散型随机变量（discrete random variable）
2. 连续型随机变量（continuous random variable）

离散变量的每个可能的取值都具有大于 0 的概率，取值和概率之间一一对应的关系就是离散型随机变量的分布律，也叫**概率质量函数（probability mass function）**。概率质量函数在连续型随机变量上的对应就是**概率密度函数（probability density function）**。
概率密度函数体现的并非连续型随机变量的真实概率，而是不同取值可能性之间的相对关系